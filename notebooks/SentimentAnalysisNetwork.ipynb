{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze text sentiment:The machine learning approach\n",
    "\n",
    "This project is based on Andrew Trask \n",
    "[Sentiment project](https://github.com/udacity/deep-learning/tree/master/sentiment-network).\n",
    "\n",
    "The dataset is part of the [Learning Word Vectors for Sentiment Analysis](http://ai.stanford.edu/~amaas/data/sentiment/) publication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import os\n",
    "import math\n",
    "from random import randint\n",
    "import sys\n",
    "import time\n",
    "from IPython.display import Image\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from lib.reviews.load_reviews import load_reviews\n",
    "from lib.reviews.get_words_indexes import get_words_indexes\n",
    "from lib.activation_functions.sigmoid import sigmoid\n",
    "from lib.derivatives.sigmoid_derivative import sigmoid_derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the reviews and labels data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "POSITIVE_DATASET_PATH = \"dataset/positive_reviews.txt\"\n",
    "positive_reviews = load_reviews(POSITIVE_DATASET_PATH)\n",
    "\n",
    "positive_reviews[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEGATIVE_DATASET_PATH = \"dataset/negative_reviews.txt\"\n",
    "negative_reviews = load_reviews(NEGATIVE_DATASET_PATH)\n",
    "\n",
    "negative_reviews[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the words counters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll create three `Counter` objects, one for words from postive reviews, one for words from negative reviews, and one for all the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_counts = Counter()\n",
    "negative_counts = Counter()\n",
    "total_counts = Counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words_count(reviews):\n",
    "    words_counts = Counter()\n",
    "    \n",
    "    for index in range(len(reviews)):\n",
    "        words = reviews[index].split(' ')\n",
    "        \n",
    "        for word in words:\n",
    "            words_counts[word] += 1\n",
    "            \n",
    "    return words_counts\n",
    "\n",
    "positive_counts = get_words_count(positive_reviews)\n",
    "negative_counts = get_words_count(negative_reviews)\n",
    "\n",
    "total_counts = positive_counts + negative_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the most common words in positive reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_counts.most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the respective most common words in negative reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_counts.most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, common words like \"the\" appear very often in both positive and negative reviews. Instead of finding the most common words in positive or negative reviews, what you really want are the words found in positive reviews more often than in negative reviews, and vice versa. To accomplish this, you'll need to calculate the ratios of word usage between positive and negative reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_neg_ratios = Counter()\n",
    "\n",
    "for word in positive_counts:\n",
    "    if(positive_counts[word] > 100 or negative_counts[word] > 100):\n",
    "        pos_neg_ratios[word] = math.log(positive_counts[word] / (negative_counts[word] + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the calculated ratios for a few words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(positive_counts[\"the\"])\n",
    "print(negative_counts[\"the\"])\n",
    "print(pos_neg_ratios[\"the\"])\n",
    "\n",
    "print(\"Pos-to-neg ratio for 'the' = {}\".format(pos_neg_ratios[\"the\"]))\n",
    "print(\"Pos-to-neg ratio for 'amazing' = {}\".format(pos_neg_ratios[\"amazing\"]))\n",
    "print(\"Pos-to-neg ratio for 'terrible' = {}\".format(pos_neg_ratios[\"terrible\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neutral word have a ratio value close to 0. Words expected to see more often in positive reviews – like \"amazing\" – have a ratio greater than 0. Words with a ratio lower than 0 were expected to be more often in negative reviews.\n",
    "Extremely positive and extremely negative words will have positive-to-negative ratios with similar magnitudes but opposite signs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign a seed to our random number generator to ensure we get reproducable results during development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The network learning rate.\n",
    "learning_rate = 0.001\n",
    "\n",
    "# The polarity cutoff to exclude values very close to 0.\n",
    "POLARITY_CUTOFF = 0.02\n",
    "\n",
    "# The early stopping value expressed in percentage for the validation \n",
    "EARLY_STOPPING_VALUE = 80\n",
    "\n",
    "# The number of single pass through whole training dataset\n",
    "EPOCHS = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the words indexes dictionary processing the positive and negative reviews and keeping only the words with a ratio greater than the polarity cutoff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = 0\n",
    "words_indexes_dictionary = {}\n",
    "\n",
    "for word in pos_neg_ratios:\n",
    "    if(abs(pos_neg_ratios[word]) > POLARITY_CUTOFF):\n",
    "        words_indexes_dictionary[word] = word_index\n",
    "        word_index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the data sets for training and testing the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEGATIVE = 0\n",
    "POSITIVE = 1\n",
    "\n",
    "reviews = []\n",
    "labels = []\n",
    "\n",
    "# Insert positive reviews\n",
    "\n",
    "reviews = positive_reviews[:]\n",
    "labels = [POSITIVE] * len(reviews)\n",
    "\n",
    "# Insert randomly negative reviews\n",
    "\n",
    "for review_index in range(len(negative_reviews)):\n",
    "    index = randint(0, len(reviews))\n",
    "    reviews.insert(index, negative_reviews[review_index])\n",
    "    labels.insert(index, NEGATIVE)\n",
    "\n",
    "train_reviews = reviews[:16000]\n",
    "valid_reviews = reviews[16000:17000]\n",
    "test_reviews = reviews[-5000:]\n",
    "\n",
    "train_labels = labels[:16000]\n",
    "valid_labels = labels[16000:17000]\n",
    "test_labels = labels[-5000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the neural network structure having only an hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_LAYER_NODES = len(words_indexes_dictionary)\n",
    "HIDDEN_LAYER_NODES = 10\n",
    "OUTPUT_LAYER_NODES = 1\n",
    "\n",
    "input_to_hidden_weights = np.zeros((INPUT_LAYER_NODES, HIDDEN_LAYER_NODES))\n",
    "hidden_to_output_weights = np.random.normal(0.0, HIDDEN_LAYER_NODES ** -0.5, \n",
    "                                            (HIDDEN_LAYER_NODES, OUTPUT_LAYER_NODES))\n",
    "\n",
    "hidden_layer = np.zeros((1, HIDDEN_LAYER_NODES))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop through all the given reviews and run a forward and backward pass, updating weights for every item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    correct_predictions = 0\n",
    "\n",
    "    for review_index in range(len(train_reviews)):\n",
    "        review = train_reviews[review_index]\n",
    "        label = train_labels[review_index]\n",
    "\n",
    "        # Prepare the list of unique word indexes found on current review\n",
    "\n",
    "        words_indexes = get_words_indexes(words_indexes_dictionary, review)\n",
    "\n",
    "        ## The forward pass through the network\n",
    "\n",
    "        # Calculate the hidden layer values with the input to hidden weights\n",
    "\n",
    "        hidden_layer = np.zeros((OUTPUT_LAYER_NODES, HIDDEN_LAYER_NODES))\n",
    "\n",
    "        for word_index in words_indexes:\n",
    "            hidden_layer += input_to_hidden_weights[word_index]\n",
    "\n",
    "        # Calculate the output value multiplying the hidden layer values by the hidden to output weights\n",
    "\n",
    "        output = hidden_layer.dot(hidden_to_output_weights)\n",
    "        output = sigmoid(output)\n",
    "\n",
    "        ## The network validation\n",
    "\n",
    "        valid_correct_predictions = 0\n",
    "\n",
    "        for valid_index in range(len(valid_reviews)):\n",
    "            valid_review = valid_reviews[valid_index]\n",
    "            valid_label = valid_labels[valid_index]\n",
    "\n",
    "            words_indexes = get_words_indexes(words_indexes_dictionary, valid_review)\n",
    "\n",
    "            hidden_layer = np.zeros((OUTPUT_LAYER_NODES, HIDDEN_LAYER_NODES))\n",
    "\n",
    "            for word_index in words_indexes:\n",
    "                hidden_layer += input_to_hidden_weights[word_index]\n",
    "\n",
    "            valid_output = hidden_layer.dot(hidden_to_output_weights)\n",
    "            valid_output = sigmoid(valid_output)\n",
    "\n",
    "            valid_error = valid_output - valid_label\n",
    "\n",
    "            if(np.abs(valid_error) < 0.5):\n",
    "                valid_correct_predictions += 1\n",
    "\n",
    "        valid_accuracy = valid_correct_predictions * 100 / len(valid_reviews)\n",
    "\n",
    "        # The training will stop when chosen performance measure stops improving\n",
    "        # to avoid overfitting\n",
    "\n",
    "        if(valid_accuracy > EARLY_STOPPING_VALUE):\n",
    "            print(\"The early stopping value has been reached during validation.\")\n",
    "            break\n",
    "\n",
    "        ## The back propagation pass\n",
    "\n",
    "        # Calculate the output error and delta\n",
    "\n",
    "        error = output - label\n",
    "        \n",
    "        output_delta = error * sigmoid_derivative(output)\n",
    "\n",
    "        # Calculate the hidden error and delta\n",
    "\n",
    "        hidden_errors = output_delta.dot(hidden_to_output_weights.T)\n",
    "        hidden_deltas = hidden_errors\n",
    "\n",
    "        # Update the network weights using the calculated deltas\n",
    "\n",
    "        hidden_to_output_weights -= hidden_layer.T.dot(output_delta) * learning_rate\n",
    "\n",
    "        for word_index in words_indexes:\n",
    "            input_to_hidden_weights[word_index] -= hidden_deltas[0] * learning_rate\n",
    "\n",
    "        # Keep track of errors and correct predictions \n",
    "        \n",
    "        if(np.abs(error) < 0.5):\n",
    "            correct_predictions += 1\n",
    "\n",
    "        accuracy = correct_predictions * 100 / float(review_index + 1)\n",
    "\n",
    "        sys.stdout.write(\"\\rCorrect predictions: \" + str(correct_predictions) + \n",
    "                         \" - Trained: \" + str(review_index) +\n",
    "                         # \" - Valid accuracy: \" + str(valid_accuracy) +\n",
    "                         \" - Testing Accuracy:\" + str(accuracy)[:4] + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the test_labels to calculate the accuracy of previous predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_predictions = 0\n",
    "\n",
    "for review_index in range(len(test_reviews)):\n",
    "    review = test_reviews[review_index]\n",
    "    label = test_labels[review_index]\n",
    "    \n",
    "    # Prepare the list of unique word indexes found on current review\n",
    "    \n",
    "    words_indexes = get_words_indexes(words_indexes_dictionary, review)\n",
    "            \n",
    "    ## The forward pass through the network\n",
    "            \n",
    "    # Calculate the hidden layer values with the input to hidden weights\n",
    "        \n",
    "    hidden_layer = np.zeros((OUTPUT_LAYER_NODES, HIDDEN_LAYER_NODES))\n",
    "    \n",
    "    for word_index in words_indexes:\n",
    "        hidden_layer += input_to_hidden_weights[word_index]\n",
    "    \n",
    "    # Calculate the output value multiplying the hidden layer values by the hidden to output weights\n",
    "    \n",
    "    output = hidden_layer.dot(hidden_to_output_weights)\n",
    "    output = sigmoid(output)\n",
    "    \n",
    "    error = output - label\n",
    "    \n",
    "    # Keep track of correct predictions\n",
    "    \n",
    "    if(np.abs(error) < 0.5):\n",
    "        correct_predictions += 1\n",
    "     \n",
    "    sys.stdout.write(\"\\rCorrect predictions: \" + str(correct_predictions) \\\n",
    "                     + \" - Trained: \" + str(review_index) \\\n",
    "                     + \" - Testing Accuracy:\" \\\n",
    "                     + str(correct_predictions * 100 / float(review_index + 1))[:4] + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
